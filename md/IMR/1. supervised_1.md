## 代码改进部分

- [x] 1. 日志更改
- [ ] 2. 加上时间计算
    - [x] 日志显示时间
    - [ ] 计算配准时间
- [ ] 3. 评价指标
- [x] 4. 修改 Loss
    - [x] 引入了 `ncc` ，代码还需要调试 —— 效果不好
    - [x] 采用 `MSE+L1loss`
    - [x] 使用 `norm2`
    - [x] **使用 `GeodesicLoss` 测地距离，效果最好**
- [x] 5. [构造数据的问题](#构造数据的问题)
- [x] 6. [直方图匹配](#直方图匹配)

## 方向
- [x] 1. 改变分辨率判断收敛速度和精度
    - [x] 改变分辨率对于内存要求变化较大，因此仅提高到148\*148*148，最终效果不好，pass
- [x] 2. 分别训练R和T —— 最终效果均不好
    - [x] 两个网络分别训练
    - [x] 计算损失时给予不同的权重
    - [x] 将平移矩阵训练为图像大小的范围，而不是 $[-1, 1]$

- [x] 3. [增加网络深度，使用 `denseblock` ](#Denseblock)
- [x] 4. 更改bn层 —— 效果不好

## 2021.06.08

1.	采用G1网络，分辨率128\*128\*128，初始学习率0.001，每经过20个epoch衰减0.1倍，迭代80次，采用SGD优化器，100对数据，分为67张训练集，33张测试集。
采用MSE误差，误差在0.01左右

```sh
net: G1
shape = 128
epoch: 100
lr_init = 0.01, decay = 0.1 every 20 epochs
```

2.	采用G2网络，深度加深，分辨率128\*128\*128，初始学习率0.01，每经过10个epoch衰减0.4倍，迭代80次，采用SGD优化器，100对数据，分为67张训练集，33张测试集。其loss下降更为平滑且迅速，内存占用85%，误差在0.005以内，但是感觉有点过拟合

```sh
net: G2
shape = 128
epoch: 100
lr_init = 0.1, decay = 0.4 every 15 epochs
init: xavier_normal_
```



## 2021.06.09

1. 采用G1网络，分辨率148\*148\*148，稍微更改了下网络结构，内存占用100%，但是效果没有2好

2. 采用G2网络，分辨率128\*128\*128，初始学习率0.01，每经过15个epoch衰减0.4倍，迭代150次，采用SGD优化器，如果采用Adam优化器，容易出现梯度爆炸，需要调节学习率或对梯度进行约束
```sh
net: G2
shape = 148
epoch: 100
lr_init = 0.1, decay = 0.4 every 15 epochs
init: xavier_normal_
```

## 2021.06.21

1. DenseBlock，MSE+L1Loss

## 2021.07.06

- 数据
    - 生成方式
	    - 旋转矩阵是以`(0, 0.5)`的高斯分布生成三个角度，然后乘起来
		- 平移矩阵是以`(0, 0.04)`的高斯分布生成，约束范围为`[±0.008, ±0.13]`，转换成像素位移后为`[±4, ±62]` (这个范围是根据实际对侧图像预估的)
	- 训练数据
		- `左侧图像为Moving`，以左侧图像为模板`生成的100张图像为Fixed`进行训练，打乱顺序取`75%为训练集，25%为测试集`

- Loss
	- 采用测地距离和MSE为loss函数，对旋转矩阵采用测地距离，平移矩阵采用MSE误差
        $$Loss = GeodesicLoss + 100 * MSELoss$$
- 网络
	- 以 `[卷积层，Dense块，翻译层]` 为基本结构，总共有5个基本结构，然后再接一个卷积将其通道数降下来- 以便输入全连接层，全连接层分别是`(8192, 1024, 512, 128, 64, 12)`
	- 两幅图像 `resize` 为`(128, 128, 128)`，合并通道后作为输入。
    - 初始通道数为`8`，每经过一个基本结构输出通道`乘以2`
	- Dense块中增长率设为8个通道，5个Dense块的层数为`[1, 4, 8, 16, 32]`，翻译层的作用是`将经过Dense块后的通道修正`。
	- 所有均采用 `BN`，实际上应该是 `IN`
- 基本设置
    - `SGD`, `momentum=0.9`, `weight_decay=1e-6`
    - 初始学习率采用 `1e-3`，每经过30个 epoch 乘以`0.3`

## 2021.07.07

发生了过拟合现象，从数据入手

## 2021.07.08

将三个坐标轴分别旋转，查找数据分布的规律

## 2021.07.16

`针对同一病人不同时间段的图像的配准`
1. 构造数据使其接近于 `GroudTruth`.
2. 修改网络输出，增加约束，使其确定为刚性变换，即预测6个数值，3个旋转角度，3个平移
3. 构造的伪标签仍然是变换矩阵，因此loss收敛性目前无法明确的判断，还需要做不同的实验进行比较分析

## 2021.07.29

`增加了两幅同一病人不同时间段的图像的配准，并使用SIFT做了预实验（其中有一幅结果太差因此去掉了）`

1. 网络结构没有发生太大变化
2. 加入FILM机制，需要做对比，但是目前还没有保存未加FILM的结果
3. 采用直方图均衡后的网络得到的结果泛化能力太差，因此决定不采用直方图均衡


## 2021.09.16

`1. 截取中间的一小部分`
`2. 增加了多幅同一病人不同时间段的图像，需要都做实验`


## 2021.09.23
发现了截取数据的问题，将 `320 -> 256`，虽然部分图像仍然有黑色像素值，因此再次缩小至 `224`。

## 2021.09.27

1. dense块的通道数设置为 `[1, 4, 16, 48]`


## 2021.10.18

编写小论文 `and` 准备开题答辩

---
---
---
## 反向传播问题

1. 构造一个简易网络
2. 将图像缩放为 $32*32*32$ 作为输入，以第一对图像为数据
3. 以MSE误差计算loss
4. 学习率设 `0.01` 与 `0.001` 均作尝试，学习率太大会过早收敛，因此调小


## 构造数据的问题

#### 1. 随机生成
初始选择num = 0.01， 旋转矩阵对角线采用0.001，应用到了参考图像
后改为num = 0.1， 旋转矩阵对角线采用0.02，应用到浮动图像来构造参考图像

```py
num = 0.1
r = torch.tensor([[
    [1-random.uniform(0, 0.02), random.uniform(-num, num), random.uniform(-num, num)],
    [random.uniform(-num, num), 1-random.uniform(0, 0.02), random.uniform(-num, num)],
    [random.uniform(-num, num), random.uniform(-num, num), 1-random.uniform(0, 0.02)]
]])
# t = torch.normal(0, 0.05, (1, 3, 1))
t = torch.tensor([random.uniform(-num, num),random.uniform(-num, num),random.uniform(-num, num)]).view(1, 3, 1)
```

该方法经讨论有误，因此重新构造

#### 2. 约束三个坐标轴角度，确定是刚性变换

- [1] [绕空间任意轴的旋转变换](http://www.jmpcrash.com/?p=871)
- [2] [Maths - Rotation Matrices](http://www.euclideanspace.com/maths/algebra/matrix/orthogonal/rotation/index.htm)
- [3] [绕任意轴旋转](https://www.cnblogs.com/graphics/archive/2012/08/10/2627458.html)（最清晰）
- [4] [旋转变换（一）旋转矩阵](https://www.cnblogs.com/zhoug2020/p/7842808.html)
- [5] 2000 Affine Transformations.pdf

**先默认旋转轴过原点**

如果旋转轴是过原点的，那么第一步和最后一步的平移操作可以省略，也就是把中间五个矩阵连乘起来，再转置一下，得到下面的绕任意轴旋转的矩阵

<div align="center"><img src="https://pic002.cnblogs.com/images/2012/64257/2012080822021225.gif
" width="40%" alt=""></div>

[5] 中的pdf提到是 $R_x * R_y * R_z$，可以认为两者无差异

至 `2021.10.08` 最准确的数据构造标准是
    $$([-0.05, 0.05], [-0.05, 0.05], [-0.05, 0.05]),
    [[-0.3, 0.1], [-0.1, 0.1], [-0.1, 0.1]]$$

#### 3. 数据类型不匹配问题

注意将其更换为 `int16` 类型，否则resize之后并没有归一化

```py
grid = F.affine_grid(trans, fix_img.size())
resample = F.grid_sample(fix_img, grid).squeeze().squeeze().type(torch.ShortTensor)
resample = resample.numpy()
```

#### 4. 增加数据，并且更改一些设置
1. 去除生成数据的灰色区域（像素为0），构造时填充一圈0，然后设置 `padding_mode` 参数为 `border`.

```py
base_array = np.zeros((483, 483, 483))

base_array[1:-1, 1:-1, 1:-1] = fix_array
base_array[0, ...] = min_pixel
base_array[-1, ...] = min_pixel
base_array[:, 0, :] = min_pixel
base_array[:, -1, :] = min_pixel
base_array[..., 0] = min_pixel
base_array[..., -1] = min_pixel

fix_img = torch.FloatTensor(base_array).unsqueeze(0).(0)


grid = F.affine_grid(trans, fix_img.size())
resample = F.grid_sample(fix_img, grid, padding_mode='border').squeeze().squeeze().type(torch.ShortTensor)
```


## Denseblock

先将其叠加，再一层过渡层将其通道修正，在实现的AirNet中使用的层数为 `layers=[1, 4, 8, 16, 32]`

- [PyTorch实现DenseNet](https://blog.csdn.net/mingxiaod/article/details/88423505)

```py
def conv_block(in_channel, out_channel):
    layer = nn.Sequential(
        nn.Conv3d(in_channel, out_channel, 3, padding=1, bias=False),
        # nn.ReLU(True), nn.LeakyReLU(0.2, True),
        nn.ReLU(True), 
        nn.InstanceNorm3d(out_channel),
    )
    return layer


def transition(in_channel, out_channel):
    trans_layer = nn.Sequential(
        nn.Conv3d(in_channel, out_channel, 1),
        nn.ReLU(True),
        nn.InstanceNorm3d(out_channel),
        nn.AvgPool3d(2, 2),
    )
    return trans_layer


class Denseblock(nn.Module):
    def __init__(self, in_channel, growth_rate, num_layers):
        super(Denseblock, self).__init__()
        block = []
        for i in range(num_layers):
            block.append(conv_block(in_channel, growth_rate))
            in_channel += growth_rate
        self.net = nn.Sequential(*block)

    def forward(self, x):
        for layer in self.net:
            out = layer(x)
            x = torch.cat((out, x), dim=1)
        return x
```


## pytorch中的问题

1. 两个Relu不能放在一起

- [Why do PyTorch not support two successive inplace ReLU operations?](https://github.com/pytorch/pytorch/issues/34429)

```py
relu1 = nn.ReLU(inplace = True)
relu2 = nn.ReLU(inplace = True)
```

在使用denseblock的时候去掉BN层，结果报了这个错误

2. Normalization 问题


<div align="center"><img src="https://img-blog.csdnimg.cn/20210621111708955.png
" width="60%" alt=""></div>

<div align="center"><img src="https://img-blog.csdnimg.cn/20181225232246856.png
" width="50%" alt=""></div>

$γ$，$β$ 指 `bn` 对应的 `scale` 和 `shift` (平移和缩放因子)

- BatchNorm：      batch方向做归一化，算N\*H*W的均值
- LayerNorm：      channel方向做归一化，算C\*H*W的均值
- InstanceNorm：  一个channel内做归一化，算H*W的均值
- GroupNorm：     将channel方向分group，然后每个group内做归一化，算(C//G)\*H*W的均值

3. Denseblock 中使用 `GN` 层

引入一个变量 `group` ，将其传入每一个函数中，设定与 `growth_rate` 和 `layers` 挂钩

## resize归一化的问题

使用skimage.transform.resize遇到了没有归一化的问题，经师兄指点

**图像数据类型为整形才会归一化，为浮点型不会归一化**

## 直方图匹配
