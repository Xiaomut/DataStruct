# 1. [目标检测](https://blog.csdn.net/yegeli/article/details/109861867)

## 1.1 R-CNN 

### 1.1.1 R-CNN

- **定义**
    - R-CNN(全称Regions with CNN features) ，是R-CNN系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。比如R-CNN pipeline中的第二步和第四步其实就属于传统的“计算机视觉”技术。使用selective search提取region proposals，使用SVM实现分类。
- **流程**
    - `预训练模型`。选择一个预训练 (pre-trained)神经网络 (如AlexNet、VGG)。
    - `重新训练全连接层`。使用需要检测的目标重新训练 (re-train)最后全连接层 (connected layer)。
    - `提取 proposals并计算CNN 特征`。利用选择性搜索 (Selective Search)算法提取所有proposals (大约2000幅images)，调整 (resize/warp)它们成固定大小，以满足 CNN输入要求 (因为全连接层的限制)，然后将feature map 保存到本地磁盘。
    - `训练SVM`。利用feature map 训练SVM来对目标和背景进行分类 (每个类一个二进制SVM)
    - `边界框回归 (Bounding boxes Regression)`。训练将输出一些校正因子的线性回归分类器
- **缺点**
    - 重复计算，每个region proposal，都需要经过一个AlexNet特征提取，为所有的RoI（region of interest）提取特征大约花费47秒，占用空间
    - selective search方法生成region proposal，对一帧图像，需要花费2秒
    - 三个模块（提取、分类、回归）是分别训练的，并且在训练时候，对于存储空间消耗较大

### 1.1.2 Fast R-CNN

- **定义**
    - Fast R-CNN是基于R-CNN和SPPnets进行的改进。SPPnets，其创新点在于只进行一次图像特征提取（而不是每个候选区域计算一次），然后根据算法，将候选区域特征图映射到整张图片特征图中
- **流程**
    - 使用selective search生成region proposal，大约2000个左右区域候选框
    - (joint training)缩放图片的scale得到图片金字塔，FP得到conv5的特征金字塔
    - (joint training)对于每个scale的每个ROI，求取映射关系，在conv5中剪裁出对应的patch。并用一个单层的SSP layer来统一到一样的尺度（对于AlexNet是6*6）
    - (joint training) 继续经过两个全连接得到特征，这特征又分别共享到两个新的全连接，连接上两个优化目标。第一个优化目标是分类，使用softmax，第二个优化目标是bbox regression，使用了一个平滑的L1-loss
    - 测试时需要加上NMS处理: 利用窗口得分分别对每一类物体进行非极大值抑制提出重叠建议框，最终得到每个类别中回归修正后的得分最高的窗口
- **改进**
    - 和RCNN相比，训练时间从84小时减少为9.5小时，测试时间从47秒减少为0.32秒。在VGG16上，Fast RCNN训练速度是RCNN的9倍，测试速度是RCNN的213倍；训练速度是SPP-net的3倍，测试速度是SPP-net的3倍
    - 加入RoI Pooling，采用一个神经网络对全图提取特征
    - 在网络中加入了多任务函数边框回归，实现了端到端的训练
- **缺点**
    - 依旧采用selective search提取region proposal（耗时2~3秒，特征提取耗时0.32秒）
    - 无法满足实时应用，没有真正实现端到端训练测试
    - 利用了GPU，但是region proposal方法是在CPU上实现的

### 1.1.3 Faster RCNN

- **定义**
    - 经过R-CNN和Fast-RCNN的积淀，Ross B.Girshick在2016年提出了新的Faster RCNN，在结构上将特征抽取、region proposal提取， bbox regression，分类都整合到了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。
- **整体流程**
    - `Conv Layers`。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的卷积/激活/池化层提取图像的特征，形成一个特征图，用于后续的RPN层和全连接层。
    - `Region Proposal Networks（RPN）`。RPN网络用于生成候选区域，该层通过softmax判断锚点（anchors）属于前景还是背景，在利用bounding box - regression（包围边框回归）获得精确的候选区域。
    - `RoI Pooling`。该层收集输入的特征图和候选区域，综合这些信息提取候选区特征图（proposal feature maps），送入后续全连接层判定目标的类别。
    - `Classification`。利用取候选区特征图计算所属类别，并再次使用边框回归算法获得边框最终的精确位置。
- **Anchors**
    Anchors（锚点）指由一组矩阵，每个矩阵对应不同的检测尺度大小。


## 1.2 YOLO系列

### 1.2.1 YOLOv1（2016）

YOLO（You Only Look Once ）是继RCNN，fast-RCNN和faster-RCNN之后，Ross Girshick针对DL目标检测速度问题提出的另一种框架，其核心思想是生成RoI+目标检测两阶段（two-stage）算法用一套网络的一阶段（one-stage）算法替代，直接在输出层回归bounding box的位置和所属类别。

之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。

YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。下图展示了各物体检测系统的流程图。

- **网络结构**
    - YOLOv1网络有24个卷积层，后面是2个全连接层。我们只使用1 × 1 1 \times 11×1降维层，后面是3 × 3 3 \times 33×3卷积层。
- **训练过程与细节**
    - 预训练。采用前20个卷积层、平均池化层、全连接层进行了大约一周的预训练；

    - 输入。输入数据为224*224和448*448大小的图像；

    - 采用相对坐标。通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间；边界框x xx和y yy坐标参数化为特定网格单元位置的偏移量，边界也在0和1之间；

    - 损失函数
        - 损失函数由坐标预测、是否包含目标物体置信度、类别预测构成；
        - 如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误；
        - 如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误
    - 学习率
    - 避免过拟合策略。使用dropout和数据增强来避免过拟合
- **优点与缺点**
    - 优点
        - YOLO检测物体速度非常快，其增强版GPU中能跑45fps（frame per second），简化版155fps
        - YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息
        - YOLO可以学到物体泛化特征
    - 缺点
        - 精度低于其它state-of-the-art的物体检测系统
        - 容易产生定位错误
        - 对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体
        - 由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体处理上还有待加强


### 1.2.2 YOLOv2（2016）

Ross Girshick吸收fast-RCNN和SSD算法，设计了YOLOv2（论文原名《YOLO9000: Better, Faster, Stronger 》），在精度上利用一些列训练技巧，在速度上应用了新的网络模型DarkNet19，在分类任务上采用联合训练方法，结合wordtree等方法，使YOLOv2的检测种类扩充到了上千种，作者在论文中称可以检测超过9000个目标类别，所以也称YOLO9000. YOLOv2模型可以以不同的尺寸运行，从而在速度和准确性之间提供了一个简单的折衷，在67FPS时，YOLOv2在VOC 2007上获得了76.8 mAP。在40FPS时，YOLOv2获得了78.6 mAP，比使用ResNet的Faster R-CNN和SSD等先进方法表现更出色，同时仍然运行速度显著更快。

- **改进策略**
    - Batch Normalization（批量正则化）
    - High Resolution Classifier（高分辨率分类器）
    - Convolutional With Anchor Boxes（带Anchor Boxes的卷积）
    - Dimension Clusters（维度聚类）
    - New Network（新的网络）
    - 直接定位预测（Direct location Prediction）
    - 细粒度特征（Fine-Grained Features）
    - 多尺度训练（Multi-Scale Training）
-  **训练过程**
    - 第一阶段: 现在ImageNet分类数据集上训练Darknet-19,此时模型输入为224*224，共训练160轮
   - 第二阶段: 将网络输入调整为448*448，继续在ImageNet分类数据集上训练细调模型，共10轮，此时分类模型top-1准确率为76.5%，而top-5准确度为93.3%
    - 第三阶段: 修改Darknet-19分类模型为检测模型，并在检测数据集上继续细调网络

### 1.2.3 YOLOv3（2018）

YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。

- **改进**
    - 新网络结构: DarkNet-53；
    - 用逻辑回归替代softmax作为分类器；
    - 融合FPN（特征金字塔网络），实现多尺度检测。

### 1.2.4 YOLOv4
### 1.2.5 YOLOv5


# 2. 常见网络

## 2.1 Resnet

- [详解残差网络](https://zhuanlan.zhihu.com/p/42706477)

在统计学中，残差和误差是非常容易混淆的两个概念。误差是衡量观测值和真实值之间的差距，残差是指预测值和观测值之间的差距。对于残差网络的命名原因，作者给出的解释是，网络的一层通常可以看做 $y=H(x)$，而残差网络的一个残差块可以表示为 $H(x)=F(x)+x$，也就是 $F(x)=H(x)-x$，在单位映射中， $$y=x 便是观测值，而 $H(x)$ 是预测值，所以 $F(x)$ 便对应着残差，因此叫做残差网络。

## 2.2 Unet


## 2.3 mobilenet
## 2.4 Attention

### 2.4.1 空间域注意力方法

对于卷积神经网络，CNN每一层都会输出一个C x H x W的特征图，C就是通道，同时也代表卷积核的数量，亦为特征的数量，H 和W就是原始图片经过压缩后的图的高度和宽度，而空间注意力就是对于所有的通道，在二维平面上，对H x W尺寸的特征图学习到一个权重矩阵，对应每个像素都会学习到一个权重。而这些权重代表的就是某个空间位置信息的重要程度 ，将该空间注意力矩阵附加在原来的特征图上，增大有用的特征，弱化无用特征，从而起到特征筛选和增强的效果。代表的Self-Attention、Non-local Attention以及Spatial Transformer等。

#### 2.4.1.1 自注意力: Self-Attention

自注意力是目前应用最广泛的注意力机制之一，self-attention及其变体广泛应用与自然语言处理、图像处理及语音识别的各个领域，特别是NLP领域，基于self-attention的Transformer结构已经成为NLP技术的基石。CV领域的self-attention也来源于NLP，甚至在某些分割、识别任务上直接套用NLP的Transformer结构并且取得了非常好的结果。

自注意力的结构下图所示，它是从NLP中借鉴过来的思想，因此仍然保留了Query, Key和Value等名称。对应图中自上而下分的三个分支，计算时通常分为三步: 

(1) 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；
(2) 第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；
(3) 第三步将权重和相应的键值value进行加权求和得到最后的attention。

自注意力是基于特征图本身的关注而提取的注意力。对于卷积而言，卷积核的设置限制了感受野的大小，导致网络往往需要多层的堆叠才能关注到整个特征图。而自注意的优势就是它的关注是全局的，它能通过简单的查询与赋值就能获取到特征图的全局空间信息。

#### 2.4.1.2 非局部注意力: Non-local Attention

Non-local Attention是研究self-attention在CV领域应用非常重要的文章。主要思想也很简单，CNN中的卷积单元每次只关注邻域kernel size 的区域，就算后期感受野越来越大，终究还是局部区域的运算，这样就忽略了全局其他片区（比如很远的像素）对当前区域的贡献。所以Non-local blocks 要做的是，捕获这种long-range 关系: 对于2D图像，就是图像中任何像素对当前像素的关系权值；对于3D视频，就是所有帧中的所有像素，对当前帧的像素的关系权值。

<div align="center"><img src="./assets/nonlocalblock.png" width="70%"> </div>

### 2.4.2 通道域注意力方法

不同与空间注意力，通道域注意力类似于<font color="red">给每个通道上的特征图都施加一个权重</font>，来代表该通道与关键信息的相关度的话，这个权重越大，则表示相关度越高。在神经网络中，越高的维度特征图尺寸越小，通道数越多，通道就代表了整个图像的特征信息。如此多的通道信息，对于神经网络来说，要甄别筛选有用的通道信息是很难的，这时如果用一个通道注意力告诉该网络哪些是重要的，往往能起到很好的效果，这时CV领域做通道注意力往往比空间好的一个原因。代表的是SENet、SKNet、ECANet等。

#### 2.4.2.1 SENet

该注意力机制主要分为三个部分: 挤压(squeeze)，激励(excitation)，以及注意(scale)。

首先是 Squeeze 操作，从空间维度来进行特征压缩，将h*w*c的特征变成一个1*1*c的特征，得到向量某种程度上具有全域性的感受野，并且输出的通道数和输入的特征通道数相匹配，它表示在特征通道上响应的全域性分布。算法很简单，就是一个全局平均池化。

其次是 Excitation 操作，通过引入 w 参数来为每个特征通道生成权重，其中 w 就是一个多层感知器，是可学习的，中间经过一个降维，减少参数量。并通过一个 Sigmoid 函数获得 0~1 之间归一化的权重，完成显式地建模特征通道间的相关性。

最后是一个 Scale 的操作，将 Excitation 的输出的权重看做是经过选择后的每个特征通道的重要性，通过通道宽度相乘加权到先前的特征上，完成在通道维度上的对原始特征的重标定。

<div align="center"><img src="./assets/SENet.png" width="70%"> </div>

```py
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)
```

#### 2.4.2.2 SKNet

设置了一组动态选择的卷积，分为三个部分操作`Split、Fuse、Select`。

<div align="center"><img src="./assets/SK conv.png" width="70%"> </div>

`Split`: 对输入向量X进行不同卷积核大小的完整卷积操作（组卷积），特别地，为了进一步提升效率，将5x5的传统卷积替代为dilation=2，卷积核为3x3的空洞卷积；

`Fuse`: 类似SE模块的处理，两个feature map相加后，进行全局平均池化操作，全连接先降维再升维的为两层全连接层，输出的两个注意力系数向量a和b，其中a+b=1；

`Select`: Select操作对应于SE模块中的Scale。Select使用a和b两个权重矩阵对之前的两个feature map进行加权操作，它们之间有一个类似于特征挑选的操作。

### 2.4.3 混合域注意力方法

#### 2.4.3.1 CBAM

给定一个中间特征图，CBAM模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图与输入特征图相乘以进行自适应特征优化

<div align="center"><img src="./assets/CBAM.png" width="70%"> </div>

空间注意力模块设计也参考了SENet，它将全局平均池化用在了通道上，因此作用后就得到了一个二维的空间注意力系数矩阵。值得注意的是，CBAM在空间与通道上同时做全局平均和全局最大的混合pooling，能够提取到更多的有效信息。

```py
class ChannelAttentionModule(nn.Module):
    def __init__(self, channel, ratio=16):
        super(ChannelAttentionModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.shared_MLP = nn.Sequential(
            nn.Conv2d(channel, channel // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(channel // ratio, channel, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = self.shared_MLP(self.avg_pool(x))
        maxout = self.shared_MLP(self.max_pool(x))
        return self.sigmoid(avgout + maxout)


class SpatialAttentionModule(nn.Module):
    def __init__(self):
        super(SpatialAttentionModule, self).__init__()
        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = torch.mean(x, dim=1, keepdim=True)
        maxout, _ = torch.max(x, dim=1, keepdim=True)
        out = torch.cat([avgout, maxout], dim=1)
        out = self.sigmoid(self.conv2d(out))
        return out


class CBAM(nn.Module):
    def __init__(self, channel):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttentionModule(channel)
        self.spatial_attention = SpatialAttentionModule()

    def forward(self, x):
        out = self.channel_attention(x) * x
        out = self.spatial_attention(out) * out
        return out
```

#### 2.4.3.2 DANet

主要包括Position Attention Module 和 Channel Attention Module。两个模块使用的方法都是self-attention，只是作用的位置不同，一个是空间域的self-attention，一个是通道域的self-attention。这样做的好处是：在CBAM 分别进行空间和通道self-attention的思想上，直接使用了non-local 的自相关矩阵Matmul 的形式进行运算，避免了CBAM 手工设计pooling，多层感知器等复杂操作。同时，把Self-attention的思想用在图像分割，可通过long-range上下文关系更好地做到精准分割。

## 2.5 Transformer



# 3. 常用方法

## 3.1 参数初始化

PyTorch 中参数的默认初始化在各个层的 `reset_parameters()` 方法中。例如: `nn.Linear` 和 `nn.Conv2D`，都是在 `[-limit, limit]` 之间的均匀分布，其中 `limit` 是 `1./sqrt(fan_in)`

- 如果将所有的权重都设置为0，则每一层激活函数的输出都是0
- 用N(0,1）标准正态的方式去初始化参数，每层激活函数的输出集中为-1或者1，这也就意味着激活函数陷入了饱和状态，在这个状态下，激活函数的导数为0
- 采用均匀分布的方式去初始化参数，结果随着层数的增加，每层激活函数的输出逐渐往0靠拢

### 3.1.1 Xavier Initialization

Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。这是通用的方法，适用于任何激活函数。

**为什么均值为0，标准差为1这么重要?**

考虑一个有100层的深度神经网络。在每一步，权重矩阵乘以来自前一层的激活。如果每一层的激活大于1，当它们被重复乘以100次时，它们就会不断变大，爆炸到无穷大。类似地，如果激活值小于1，它们将消失为零。这叫做渐变爆炸和渐变消失问题。我们可以从下图中看到这一点。甚至比1稍大一点的值也会爆炸成非常大的数字，而比1稍小一点的值也会消失为零。


**xavier初始化方法中服从均匀分布 $U(-a, a)$**
$$a= gain * \sqrt\frac{6}{fan_{in} + fan_{out}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系

**xavier初始化方法中服从均匀分布 $N(0, std)$**
$$std= gain * \sqrt\frac{2}{fan_{in} + fan_{out}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系

### 3.1.2 Kaiming Initialization

**kaiming初始化方法中服从均匀分布 $U(−bound,bound)$**
$$bound=  \sqrt\frac{2}{(1+a^2) × fan_{in}}$$

`a` 是激活函数的负斜率（对于`leaky_relu`来说），如果激活函数是`relu`的话，`a` 为 `0`。`mode` 默认为fan_in模式，可以设置为fan_out模式。fan_in可以保持前向传播的权重方差的数量级，fan_out可以保持反向传播的权重方差的数量级

**kaiming初始化方法中服从均匀分布 $N(0, std)$**
$$std=  \sqrt\frac{2}{(1+a^2) × fan_{in}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系


## 3.2 优化算法

定义: 待优化参数: w ，目标函数:  f(w)，初始学习率 α。而后，开始进行迭代优化。在每个epoch t: 

1. 计算目标函数关于当前参数的梯度:  
2. 根据历史梯度计算一阶动量和二阶动量: 
3. 计算当前时刻的下降梯度:  
4. 根据下降梯度进行更新:   

- [Adam与SGD](https://www.cnblogs.com/jiangkejie/p/10561100.html)
### 3.2.1 SGD
### 3.2.2 Adam


## 3.3 卷积方式

### 3.3.1 Group Convolution


<div align="center"><img src="./assets/group conv.png" width="70%"> </div>

`Group Convolution` 顾名思义，则是对输入 `feature map` 进行分组，然后每组分别卷积。假设输入 `feature map` 的尺寸仍为 $C∗H∗W$，输出 `feature map` 的数量为 $N$个，如果设定要分成$G$个 groups，则每组的输入 `feature map` 数量为$\frac{C}{G}$，每组的输出 `feature map` 数量为$\frac{N}{G}$，每个卷积核的尺寸为$\frac{C}{G} * K * K$，卷积核的总数仍为 $N$ 个，每组的卷积核数量为 $\frac{N}{G}$，卷积核只与其同组的输入map进行卷积，卷积核的总参数量为 $N * \frac{C}{G} * K * K$，可见，总参数量减少为原来的 $\frac{1}{G}$，其连接方式如上图右所示，group1输出map数为2，有2个卷积核，每个卷积核的channel数为4，与group1的输入map的channel数相同，卷积核只与同组的输入map卷积，而不与其他组的输入map卷积。

1. **减少参数量**，分成GG组，则该层的参数量减少为原来的1G1G
2. **Group Convolution可以看成是structured sparse**，每个卷积核的尺寸由C∗K∗KC∗K∗K变为CG∗K∗KCG∗K∗K，可以将其余(C−CG)∗K∗K(C−CG)∗K∗K的参数视为0，有时甚至可以在减少参数量的同时获得更好的效果（相当于**正则**）。
3. 当分组数量等于输入map数量，输出map数量也等于输入map数量，即G=N=CG=N=C、NN个卷积核每个尺寸为1∗K∗K1∗K∗K时，Group Convolution就成了**Depthwise Convolution**，参见MobileNet和Xception等，**参数量进一步缩减**，如下图所示

<div align="center"><img src="./assets/depthwise conv.png" width="70%"> </div>

4. 更进一步，如果分组数 $G=N=C$，同时卷积核的尺寸与输入map的尺寸相同，即 $K=H=W$，则输出map为 $C∗1∗1$ 即长度为 $C$ 的向量，此时称之为 **Global Depthwise Convolution（GDC）**，见MobileFaceNet，可以看成是**全局加权池化**，与 **Global Average Pooling（GAP）** 的不同之处在于，GDC 给**每个位置赋予了可学习的权重**（对于已对齐的图像这很有效，比如人脸，中心位置和边界位置的权重自然应该不同），而GAP每个位置的权重相同，全局取个平均，如下图所示: 

<div align="center"><img src="./assets/GDC.png" width="70%"> </div>

### 3.3.2 Depthwise Convolution

深度可分离卷积主要分为两个过程，分别为逐通道卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）。


#### 3.3.2.1 逐通道卷积（Depthwise Convolution）

Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积，这个过程产生的feature map通道数和输入的通道数完全一样。

Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。

#### 3.3.2.2 逐点卷积（Pointwise Convolution）

Pointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map。（卷积核的shape即为：**1 x 1 x 输入通道数 x 输出通道数**）

### 3.3.3 Dilated Convolution

网络中加入pooling层会损失信息，降低精度。那么不加pooling层会使感受野变小，学不到全局的特征。如果我们单纯的去掉pooling层、扩大卷积核的话，这样纯粹的扩大卷积核势必导致计算量的增大，此时最好的办法就是Dilated Convolutions（扩张卷积或叫空洞卷积）。

<div align="center"><img src="./assets/dilation conv.gif" width="70%"> </div>

dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv，比如图像分割、语音合成WaveNet、机器翻译ByteNet中。