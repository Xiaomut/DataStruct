# 1. [目标检测](https://blog.csdn.net/yegeli/article/details/109861867)

## 1.1 R-CNN 

### 1.1.1 R-CNN

- **定义**
    - R-CNN(全称Regions with CNN features) ，是R-CNN系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。比如R-CNN pipeline中的第二步和第四步其实就属于传统的“计算机视觉”技术。使用selective search提取region proposals，使用SVM实现分类。
- **流程**
    - `预训练模型`。选择一个预训练 (pre-trained)神经网络 (如AlexNet、VGG)。
    - `重新训练全连接层`。使用需要检测的目标重新训练 (re-train)最后全连接层 (connected layer)。
    - `提取 proposals并计算CNN 特征`。利用选择性搜索 (Selective Search)算法提取所有proposals (大约2000幅images)，调整 (resize/warp)它们成固定大小，以满足 CNN输入要求 (因为全连接层的限制)，然后将feature map 保存到本地磁盘。
    - `训练SVM`。利用feature map 训练SVM来对目标和背景进行分类 (每个类一个二进制SVM)
    - `边界框回归 (Bounding boxes Regression)`。训练将输出一些校正因子的线性回归分类器
- **缺点**
    - 重复计算，每个region proposal，都需要经过一个AlexNet特征提取，为所有的RoI（region of interest）提取特征大约花费47秒，占用空间
    - selective search方法生成region proposal，对一帧图像，需要花费2秒
    - 三个模块（提取、分类、回归）是分别训练的，并且在训练时候，对于存储空间消耗较大

### 1.1.2 Fast R-CNN

- **定义**
    - Fast R-CNN是基于R-CNN和SPPnets进行的改进。SPPnets，其创新点在于只进行一次图像特征提取（而不是每个候选区域计算一次），然后根据算法，将候选区域特征图映射到整张图片特征图中
- **流程**
    - 使用selective search生成region proposal，大约2000个左右区域候选框
    - (joint training)缩放图片的scale得到图片金字塔，FP得到conv5的特征金字塔
    - (joint training)对于每个scale的每个ROI，求取映射关系，在conv5中剪裁出对应的patch。并用一个单层的SSP layer来统一到一样的尺度（对于AlexNet是6*6）
    - (joint training) 继续经过两个全连接得到特征，这特征又分别共享到两个新的全连接，连接上两个优化目标。第一个优化目标是分类，使用softmax，第二个优化目标是bbox regression，使用了一个平滑的L1-loss
    - 测试时需要加上NMS处理：利用窗口得分分别对每一类物体进行非极大值抑制提出重叠建议框，最终得到每个类别中回归修正后的得分最高的窗口
- **改进**
    - 和RCNN相比，训练时间从84小时减少为9.5小时，测试时间从47秒减少为0.32秒。在VGG16上，Fast RCNN训练速度是RCNN的9倍，测试速度是RCNN的213倍；训练速度是SPP-net的3倍，测试速度是SPP-net的3倍
    - 加入RoI Pooling，采用一个神经网络对全图提取特征
    - 在网络中加入了多任务函数边框回归，实现了端到端的训练
- **缺点**
    - 依旧采用selective search提取region proposal（耗时2~3秒，特征提取耗时0.32秒）
    - 无法满足实时应用，没有真正实现端到端训练测试
    - 利用了GPU，但是region proposal方法是在CPU上实现的

### 1.1.3 Faster RCNN

- **定义**
    - 经过R-CNN和Fast-RCNN的积淀，Ross B.Girshick在2016年提出了新的Faster RCNN，在结构上将特征抽取、region proposal提取， bbox regression，分类都整合到了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。
- **整体流程**
    - `Conv Layers`。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的卷积/激活/池化层提取图像的特征，形成一个特征图，用于后续的RPN层和全连接层。
    - `Region Proposal Networks（RPN）`。RPN网络用于生成候选区域，该层通过softmax判断锚点（anchors）属于前景还是背景，在利用bounding box - regression（包围边框回归）获得精确的候选区域。
    - `RoI Pooling`。该层收集输入的特征图和候选区域，综合这些信息提取候选区特征图（proposal feature maps），送入后续全连接层判定目标的类别。
    - `Classification`。利用取候选区特征图计算所属类别，并再次使用边框回归算法获得边框最终的精确位置。
- **Anchors**
    Anchors（锚点）指由一组矩阵，每个矩阵对应不同的检测尺度大小。


## 1.2 YOLO系列

### 1.2.1 YOLOv1（2016）

YOLO（You Only Look Once ）是继RCNN，fast-RCNN和faster-RCNN之后，Ross Girshick针对DL目标检测速度问题提出的另一种框架，其核心思想是生成RoI+目标检测两阶段（two-stage）算法用一套网络的一阶段（one-stage）算法替代，直接在输出层回归bounding box的位置和所属类别。

之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。

YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。下图展示了各物体检测系统的流程图。

- **网络结构**
    - YOLOv1网络有24个卷积层，后面是2个全连接层。我们只使用1 × 1 1 \times 11×1降维层，后面是3 × 3 3 \times 33×3卷积层。
- **训练过程与细节**
    - 预训练。采用前20个卷积层、平均池化层、全连接层进行了大约一周的预训练；

    - 输入。输入数据为224*224和448*448大小的图像；

    - 采用相对坐标。通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间；边界框x xx和y yy坐标参数化为特定网格单元位置的偏移量，边界也在0和1之间；

    - 损失函数
        - 损失函数由坐标预测、是否包含目标物体置信度、类别预测构成；
        - 如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误；
        - 如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误
    - 学习率
    - 避免过拟合策略。使用dropout和数据增强来避免过拟合
- **优点与缺点**
    - 优点
        - YOLO检测物体速度非常快，其增强版GPU中能跑45fps（frame per second），简化版155fps
        - YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息
        - YOLO可以学到物体泛化特征
    - 缺点
        - 精度低于其它state-of-the-art的物体检测系统
        - 容易产生定位错误
        - 对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体
        - 由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体处理上还有待加强


### 1.2.2 YOLOv2（2016）

Ross Girshick吸收fast-RCNN和SSD算法，设计了YOLOv2（论文原名《YOLO9000: Better, Faster, Stronger 》），在精度上利用一些列训练技巧，在速度上应用了新的网络模型DarkNet19，在分类任务上采用联合训练方法，结合wordtree等方法，使YOLOv2的检测种类扩充到了上千种，作者在论文中称可以检测超过9000个目标类别，所以也称YOLO9000. YOLOv2模型可以以不同的尺寸运行，从而在速度和准确性之间提供了一个简单的折衷，在67FPS时，YOLOv2在VOC 2007上获得了76.8 mAP。在40FPS时，YOLOv2获得了78.6 mAP，比使用ResNet的Faster R-CNN和SSD等先进方法表现更出色，同时仍然运行速度显著更快。

- **改进策略**
    - Batch Normalization（批量正则化）
    - High Resolution Classifier（高分辨率分类器）
    - Convolutional With Anchor Boxes（带Anchor Boxes的卷积）
    - Dimension Clusters（维度聚类）
    - New Network（新的网络）
    - 直接定位预测（Direct location Prediction）
    - 细粒度特征（Fine-Grained Features）
    - 多尺度训练（Multi-Scale Training）
-  **训练过程**
    - 第一阶段：现在ImageNet分类数据集上训练Darknet-19,此时模型输入为224*224，共训练160轮
   - 第二阶段：将网络输入调整为448*448，继续在ImageNet分类数据集上训练细调模型，共10轮，此时分类模型top-1准确率为76.5%，而top-5准确度为93.3%
    - 第三阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续细调网络

### 1.2.3 YOLOv3（2018）

YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。

- **改进**
    - 新网络结构：DarkNet-53；
    - 用逻辑回归替代softmax作为分类器；
    - 融合FPN（特征金字塔网络），实现多尺度检测。

### 1.2.4 YOLOv4
### 1.2.5 YOLOv5


# 2. 常见网络

## 2.1 Resnet

## 2.2 Unet


## 2.3 mobilenet
## 2.4 Attention

### 2.4.1 空间域注意力方法

对于卷积神经网络，CNN每一层都会输出一个C x H x W的特征图，C就是通道，同时也代表卷积核的数量，亦为特征的数量，H 和W就是原始图片经过压缩后的图的高度和宽度，而空间注意力就是对于所有的通道，在二维平面上，对H x W尺寸的特征图学习到一个权重矩阵，对应每个像素都会学习到一个权重。而这些权重代表的就是某个空间位置信息的重要程度 ，将该空间注意力矩阵附加在原来的特征图上，增大有用的特征，弱化无用特征，从而起到特征筛选和增强的效果。代表的Self-Attention、Non-local Attention以及Spatial Transformer等。

#### 2.4.1.1 自注意力：Self-Attention

自注意力是目前应用最广泛的注意力机制之一，self-attention及其变体广泛应用与自然语言处理、图像处理及语音识别的各个领域，特别是NLP领域，基于self-attention的Transformer结构已经成为NLP技术的基石。CV领域的self-attention也来源于NLP，甚至在某些分割、识别任务上直接套用NLP的Transformer结构并且取得了非常好的结果。

自注意力的结构下图所示，它是从NLP中借鉴过来的思想，因此仍然保留了Query, Key和Value等名称。对应图中自上而下分的三个分支，计算时通常分为三步：

(1) 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；
(2) 第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；
(3) 第三步将权重和相应的键值value进行加权求和得到最后的attention。

自注意力是基于特征图本身的关注而提取的注意力。对于卷积而言，卷积核的设置限制了感受野的大小，导致网络往往需要多层的堆叠才能关注到整个特征图。而自注意的优势就是它的关注是全局的，它能通过简单的查询与赋值就能获取到特征图的全局空间信息。

#### 2.4.1.2 非局部注意力：Non-local Attention

Non-local Attention是研究self-attention在CV领域应用非常重要的文章。主要思想也很简单，CNN中的卷积单元每次只关注邻域kernel size 的区域，就算后期感受野越来越大，终究还是局部区域的运算，这样就忽略了全局其他片区（比如很远的像素）对当前区域的贡献。所以Non-local blocks 要做的是，捕获这种long-range 关系：对于2D图像，就是图像中任何像素对当前像素的关系权值；对于3D视频，就是所有帧中的所有像素，对当前帧的像素的关系权值。

<div align="center"><img src="./assets/nonlocalblock.png" width="70%"> </div>

### 2.4.2 通道域注意力方法

不同与空间注意力，通道域注意力类似于<font color="red">给每个通道上的特征图都施加一个权重</font>，来代表该通道与关键信息的相关度的话，这个权重越大，则表示相关度越高。在神经网络中，越高的维度特征图尺寸越小，通道数越多，通道就代表了整个图像的特征信息。如此多的通道信息，对于神经网络来说，要甄别筛选有用的通道信息是很难的，这时如果用一个通道注意力告诉该网络哪些是重要的，往往能起到很好的效果，这时CV领域做通道注意力往往比空间好的一个原因。代表的是SENet、SKNet、ECANet等。

#### 2.4.2.1 SENet

<div align="center"><img src="./assets/SENet.png" width="70%"> </div>

# 3. 常用方法

## 3.1 参数初始化

PyTorch 中参数的默认初始化在各个层的 `reset_parameters()` 方法中。例如：`nn.Linear` 和 `nn.Conv2D`，都是在 `[-limit, limit]` 之间的均匀分布，其中 `limit` 是 `1./sqrt(fan_in)`

- 如果将所有的权重都设置为0，则每一层激活函数的输出都是0
- 用N(0,1）标准正态的方式去初始化参数，每层激活函数的输出集中为-1或者1，这也就意味着激活函数陷入了饱和状态，在这个状态下，激活函数的导数为0
- 采用均匀分布的方式去初始化参数，结果随着层数的增加，每层激活函数的输出逐渐往0靠拢

### 3.1.1 Xavier Initialization

Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。这是通用的方法，适用于任何激活函数。

**为什么均值为0，标准差为1这么重要?**

考虑一个有100层的深度神经网络。在每一步，权重矩阵乘以来自前一层的激活。如果每一层的激活大于1，当它们被重复乘以100次时，它们就会不断变大，爆炸到无穷大。类似地，如果激活值小于1，它们将消失为零。这叫做渐变爆炸和渐变消失问题。我们可以从下图中看到这一点。甚至比1稍大一点的值也会爆炸成非常大的数字，而比1稍小一点的值也会消失为零。


**xavier初始化方法中服从均匀分布 $U(-a, a)$**
$$a= gain * \sqrt\frac{6}{fan_{in} + fan_{out}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系

**xavier初始化方法中服从均匀分布 $N(0, std)$**
$$std= gain * \sqrt\frac{2}{fan_{in} + fan_{out}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系

### 3.1.2 Kaiming Initialization

**kaiming初始化方法中服从均匀分布 $U(−bound,bound)$**
$$bound=  \sqrt\frac{2}{(1+a^2) × fan_{in}}$$

`a` 是激活函数的负斜率（对于`leaky_relu`来说），如果激活函数是`relu`的话，`a` 为 `0`。`mode` 默认为fan_in模式，可以设置为fan_out模式。fan_in可以保持前向传播的权重方差的数量级，fan_out可以保持反向传播的权重方差的数量级

**kaiming初始化方法中服从均匀分布 $N(0, std)$**
$$std=  \sqrt\frac{2}{(1+a^2) × fan_{in}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系
