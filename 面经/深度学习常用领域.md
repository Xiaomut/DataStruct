# 1. [目标检测](https://blog.csdn.net/yegeli/article/details/109861867)

## 1.1 R-CNN 

### 1.1.1 R-CNN

- **定义**
    - R-CNN(全称Regions with CNN features) ，是R-CNN系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。比如R-CNN pipeline中的第二步和第四步其实就属于传统的“计算机视觉”技术。使用selective search提取region proposals，使用SVM实现分类。
- **流程**
    - `预训练模型`。选择一个预训练 (pre-trained)神经网络 (如AlexNet、VGG)。
    - `重新训练全连接层`。使用需要检测的目标重新训练 (re-train)最后全连接层 (connected layer)。
    - `提取 proposals并计算CNN 特征`。利用选择性搜索 (Selective Search)算法提取所有proposals (大约2000幅images)，调整 (resize/warp)它们成固定大小，以满足 CNN输入要求 (因为全连接层的限制)，然后将feature map 保存到本地磁盘。
    - `训练SVM`。利用feature map 训练SVM来对目标和背景进行分类 (每个类一个二进制SVM)
    - `边界框回归 (Bounding boxes Regression)`。训练将输出一些校正因子的线性回归分类器
- **缺点**
    - 重复计算，每个region proposal，都需要经过一个AlexNet特征提取，为所有的RoI（region of interest）提取特征大约花费47秒，占用空间
    - selective search方法生成region proposal，对一帧图像，需要花费2秒
    - 三个模块（提取、分类、回归）是分别训练的，并且在训练时候，对于存储空间消耗较大

### 1.1.2 Fast R-CNN

- **定义**
    - Fast R-CNN是基于R-CNN和SPPnets进行的改进。SPPnets，其创新点在于只进行一次图像特征提取（而不是每个候选区域计算一次），然后根据算法，将候选区域特征图映射到整张图片特征图中
- **流程**
    - 使用selective search生成region proposal，大约2000个左右区域候选框
    - (joint training)缩放图片的scale得到图片金字塔，FP得到conv5的特征金字塔
    - (joint training)对于每个scale的每个ROI，求取映射关系，在conv5中剪裁出对应的patch。并用一个单层的SSP layer来统一到一样的尺度（对于AlexNet是6*6）
    - (joint training) 继续经过两个全连接得到特征，这特征又分别共享到两个新的全连接，连接上两个优化目标。第一个优化目标是分类，使用softmax，第二个优化目标是bbox regression，使用了一个平滑的L1-loss
    - 测试时需要加上NMS处理: 利用窗口得分分别对每一类物体进行非极大值抑制提出重叠建议框，最终得到每个类别中回归修正后的得分最高的窗口
- **改进**
    - 和RCNN相比，训练时间从84小时减少为9.5小时，测试时间从47秒减少为0.32秒。在VGG16上，Fast RCNN训练速度是RCNN的9倍，测试速度是RCNN的213倍；训练速度是SPP-net的3倍，测试速度是SPP-net的3倍
    - 加入RoI Pooling，采用一个神经网络对全图提取特征
    - 在网络中加入了多任务函数边框回归，实现了端到端的训练
- **缺点**
    - 依旧采用selective search提取region proposal（耗时2~3秒，特征提取耗时0.32秒）
    - 无法满足实时应用，没有真正实现端到端训练测试
    - 利用了GPU，但是region proposal方法是在CPU上实现的

### 1.1.3 Faster RCNN

- **定义**
    - 经过R-CNN和Fast-RCNN的积淀，Ross B.Girshick在2016年提出了新的Faster RCNN，在结构上将特征抽取、region proposal提取， bbox regression，分类都整合到了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。
- **整体流程**
    - `Conv Layers`。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的卷积/激活/池化层提取图像的特征，形成一个特征图，用于后续的RPN层和全连接层。
    - `Region Proposal Networks（RPN）`。RPN网络用于生成候选区域，该层通过softmax判断锚点（anchors）属于前景还是背景，在利用bounding box - regression（包围边框回归）获得精确的候选区域。
    - `RoI Pooling`。该层收集输入的特征图和候选区域，综合这些信息提取候选区特征图（proposal feature maps），送入后续全连接层判定目标的类别。
    - `Classification`。利用取候选区特征图计算所属类别，并再次使用边框回归算法获得边框最终的精确位置。
- **Anchors**
    Anchors（锚点）指由一组矩阵，每个矩阵对应不同的检测尺度大小。


## 1.2 YOLO系列

### 1.2.1 YOLOv1（2016）

YOLO（You Only Look Once ）是继RCNN，fast-RCNN和faster-RCNN之后，Ross Girshick针对DL目标检测速度问题提出的另一种框架，其核心思想是生成RoI+目标检测两阶段（two-stage）算法用一套网络的一阶段（one-stage）算法替代，直接在输出层回归bounding box的位置和所属类别。

之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。

YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。下图展示了各物体检测系统的流程图。

- **网络结构**
    - YOLOv1网络有24个卷积层，后面是2个全连接层。我们只使用1 × 1 1 \times 11×1降维层，后面是3 × 3 3 \times 33×3卷积层。
- **训练过程与细节**
    - 预训练。采用前20个卷积层、平均池化层、全连接层进行了大约一周的预训练；

    - 输入。输入数据为224*224和448*448大小的图像；

    - 采用相对坐标。通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间；边界框x xx和y yy坐标参数化为特定网格单元位置的偏移量，边界也在0和1之间；

    - 损失函数
        - 损失函数由坐标预测、是否包含目标物体置信度、类别预测构成；
        - 如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误；
        - 如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误
    - 学习率
    - 避免过拟合策略。使用dropout和数据增强来避免过拟合
- **优点与缺点**
    - 优点
        - YOLO检测物体速度非常快，其增强版GPU中能跑45fps（frame per second），简化版155fps
        - YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息
        - YOLO可以学到物体泛化特征
    - 缺点
        - 精度低于其它state-of-the-art的物体检测系统
        - 容易产生定位错误
        - 对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体
        - 由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体处理上还有待加强


### 1.2.2 YOLOv2（2016）

Ross Girshick吸收fast-RCNN和SSD算法，设计了YOLOv2（论文原名《YOLO9000: Better, Faster, Stronger 》），在精度上利用一些列训练技巧，在速度上应用了新的网络模型DarkNet19，在分类任务上采用联合训练方法，结合wordtree等方法，使YOLOv2的检测种类扩充到了上千种，作者在论文中称可以检测超过9000个目标类别，所以也称YOLO9000. YOLOv2模型可以以不同的尺寸运行，从而在速度和准确性之间提供了一个简单的折衷，在67FPS时，YOLOv2在VOC 2007上获得了76.8 mAP。在40FPS时，YOLOv2获得了78.6 mAP，比使用ResNet的Faster R-CNN和SSD等先进方法表现更出色，同时仍然运行速度显著更快。

- **改进策略**
    - Batch Normalization（批量正则化）
    - High Resolution Classifier（高分辨率分类器）
    - Convolutional With Anchor Boxes（带Anchor Boxes的卷积）
    - Dimension Clusters（维度聚类）
    - New Network（新的网络）
    - 直接定位预测（Direct location Prediction）
    - 细粒度特征（Fine-Grained Features）
    - 多尺度训练（Multi-Scale Training）
-  **训练过程**
    - 第一阶段: 现在ImageNet分类数据集上训练Darknet-19,此时模型输入为224*224，共训练160轮
   - 第二阶段: 将网络输入调整为448*448，继续在ImageNet分类数据集上训练细调模型，共10轮，此时分类模型top-1准确率为76.5%，而top-5准确度为93.3%
    - 第三阶段: 修改Darknet-19分类模型为检测模型，并在检测数据集上继续细调网络

### 1.2.3 YOLOv3（2018）

YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。

- **改进**
    - 新网络结构: DarkNet-53；
    - 用逻辑回归替代softmax作为分类器；
    - 融合FPN（特征金字塔网络），实现多尺度检测。

### 1.2.4 YOLOv4
### 1.2.5 YOLOv5


# 2. 常见网络

## 2.1 Resnet

- [详解残差网络](https://zhuanlan.zhihu.com/p/42706477)

**残差网络可以缓解层加深带来的网络退化问题**

在统计学中，残差和误差是非常容易混淆的两个概念。误差是衡量观测值和真实值之间的差距，残差是指预测值和观测值之间的差距。对于残差网络的命名原因，作者给出的解释是，网络的一层通常可以看做 $y=H(x)$，而残差网络的一个残差块可以表示为 $H(x)=F(x)+x$，也就是 $F(x)=H(x)-x$，在单位映射中， $y=x$ 便是观测值，而 $H(x)$ 是预测值，所以 $F(x)$ 便对应着残差，因此叫做残差网络。

## 2.2 Unet


## 2.3 mobilenet
## 2.4 Attention

### 2.4.1 空间域注意力方法

对于卷积神经网络，CNN每一层都会输出一个C x H x W的特征图，C就是通道，同时也代表卷积核的数量，亦为特征的数量，H 和W就是原始图片经过压缩后的图的高度和宽度，而空间注意力就是对于所有的通道，在二维平面上，对H x W尺寸的特征图学习到一个权重矩阵，对应每个像素都会学习到一个权重。而这些权重代表的就是某个空间位置信息的重要程度 ，将该空间注意力矩阵附加在原来的特征图上，增大有用的特征，弱化无用特征，从而起到特征筛选和增强的效果。代表的Self-Attention、Non-local Attention以及Spatial Transformer等。

#### 2.4.1.1 自注意力: Self-Attention

自注意力是目前应用最广泛的注意力机制之一，self-attention及其变体广泛应用与自然语言处理、图像处理及语音识别的各个领域，特别是NLP领域，基于self-attention的Transformer结构已经成为NLP技术的基石。CV领域的self-attention也来源于NLP，甚至在某些分割、识别任务上直接套用NLP的Transformer结构并且取得了非常好的结果。

自注意力的结构下图所示，它是从NLP中借鉴过来的思想，因此仍然保留了Query, Key和Value等名称。对应图中自上而下分的三个分支，计算时通常分为三步: 

(1) 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；
(2) 第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；
(3) 第三步将权重和相应的键值value进行加权求和得到最后的attention。

自注意力是基于特征图本身的关注而提取的注意力。对于卷积而言，卷积核的设置限制了感受野的大小，导致网络往往需要多层的堆叠才能关注到整个特征图。而自注意的优势就是它的关注是全局的，它能通过简单的查询与赋值就能获取到特征图的全局空间信息。

#### 2.4.1.2 非局部注意力: Non-local Attention

Non-local Attention是研究self-attention在CV领域应用非常重要的文章。主要思想也很简单，CNN中的卷积单元每次只关注邻域kernel size 的区域，就算后期感受野越来越大，终究还是局部区域的运算，这样就忽略了全局其他片区（比如很远的像素）对当前区域的贡献。所以Non-local blocks 要做的是，捕获这种long-range 关系: 对于2D图像，就是图像中任何像素对当前像素的关系权值；对于3D视频，就是所有帧中的所有像素，对当前帧的像素的关系权值。

<div align="center"><img src="./assets/nonlocalblock.png" width="70%"> </div>

### 2.4.2 通道域注意力方法

不同与空间注意力，通道域注意力类似于<font color="red">给每个通道上的特征图都施加一个权重</font>，来代表该通道与关键信息的相关度的话，这个权重越大，则表示相关度越高。在神经网络中，越高的维度特征图尺寸越小，通道数越多，通道就代表了整个图像的特征信息。如此多的通道信息，对于神经网络来说，要甄别筛选有用的通道信息是很难的，这时如果用一个通道注意力告诉该网络哪些是重要的，往往能起到很好的效果，这时CV领域做通道注意力往往比空间好的一个原因。代表的是SENet、SKNet、ECANet等。

#### 2.4.2.1 SENet

该注意力机制主要分为三个部分: 挤压(squeeze)，激励(excitation)，以及注意(scale)。

首先是 Squeeze 操作，从空间维度来进行特征压缩，将h*w*c的特征变成一个1*1*c的特征，得到向量某种程度上具有全域性的感受野，并且输出的通道数和输入的特征通道数相匹配，它表示在特征通道上响应的全域性分布。算法很简单，就是一个全局平均池化。

其次是 Excitation 操作，通过引入 w 参数来为每个特征通道生成权重，其中 w 就是一个多层感知器，是可学习的，中间经过一个降维，减少参数量。并通过一个 Sigmoid 函数获得 0~1 之间归一化的权重，完成显式地建模特征通道间的相关性。

最后是一个 Scale 的操作，将 Excitation 的输出的权重看做是经过选择后的每个特征通道的重要性，通过通道宽度相乘加权到先前的特征上，完成在通道维度上的对原始特征的重标定。

<div align="center"><img src="./assets/SENet.png" width="70%"> </div>

```py
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)
```

#### 2.4.2.2 SKNet

设置了一组动态选择的卷积，分为三个部分操作`Split、Fuse、Select`。

<div align="center"><img src="./assets/SK conv.png" width="70%"> </div>

`Split`: 对输入向量X进行不同卷积核大小的完整卷积操作（组卷积），特别地，为了进一步提升效率，将5x5的传统卷积替代为dilation=2，卷积核为3x3的空洞卷积；

`Fuse`: 类似SE模块的处理，两个feature map相加后，进行全局平均池化操作，全连接先降维再升维的为两层全连接层，输出的两个注意力系数向量a和b，其中a+b=1；

`Select`: Select操作对应于SE模块中的Scale。Select使用a和b两个权重矩阵对之前的两个feature map进行加权操作，它们之间有一个类似于特征挑选的操作。

### 2.4.3 混合域注意力方法

#### 2.4.3.1 CBAM

给定一个中间特征图，CBAM模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图与输入特征图相乘以进行自适应特征优化

<div align="center"><img src="./assets/CBAM.png" width="70%"> </div>

空间注意力模块设计也参考了SENet，它将全局平均池化用在了通道上，因此作用后就得到了一个二维的空间注意力系数矩阵。值得注意的是，CBAM在空间与通道上同时做全局平均和全局最大的混合pooling，能够提取到更多的有效信息。

```py
class ChannelAttentionModule(nn.Module):
    def __init__(self, channel, ratio=16):
        super(ChannelAttentionModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.shared_MLP = nn.Sequential(
            nn.Conv2d(channel, channel // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(channel // ratio, channel, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = self.shared_MLP(self.avg_pool(x))
        maxout = self.shared_MLP(self.max_pool(x))
        return self.sigmoid(avgout + maxout)


class SpatialAttentionModule(nn.Module):
    def __init__(self):
        super(SpatialAttentionModule, self).__init__()
        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = torch.mean(x, dim=1, keepdim=True)
        maxout, _ = torch.max(x, dim=1, keepdim=True)
        out = torch.cat([avgout, maxout], dim=1)
        out = self.sigmoid(self.conv2d(out))
        return out


class CBAM(nn.Module):
    def __init__(self, channel):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttentionModule(channel)
        self.spatial_attention = SpatialAttentionModule()

    def forward(self, x):
        out = self.channel_attention(x) * x
        out = self.spatial_attention(out) * out
        return out
```

#### 2.4.3.2 DANet

主要包括Position Attention Module 和 Channel Attention Module。两个模块使用的方法都是self-attention，只是作用的位置不同，一个是空间域的self-attention，一个是通道域的self-attention。这样做的好处是: 在CBAM 分别进行空间和通道self-attention的思想上，直接使用了non-local 的自相关矩阵Matmul 的形式进行运算，避免了CBAM 手工设计pooling，多层感知器等复杂操作。同时，把Self-attention的思想用在图像分割，可通过long-range上下文关系更好地做到精准分割。

## 2.5 Transformer

[图解Transformer](https://cloud.tencent.com/developer/article/1745002)

<div align="center"><img src="./assets/transformer model.png" width="70%"> </div>


**Encoder**

encoder由6层相同的层组成，每一层分别由两部分组成: 

- `multi-head self-attention mechanism`
- `position-wise feed-forward network`

两个部分，都有一个**残差连接**(residual connection)，然后接着一个 **LN**

**Decoder**

encoder由6层相同的层组成，每一层分别由两部分组成: 

- `multi-head self-attention mechanism`
- `multi-head context-attention mechanism`
- `position-wise feed-forward network`

<div align="center"><img src="./assets/decoder.png" width="70%"> </div>

context-attention 是 encoder 和 decoder 之间的 attention，也可以称之为`encoder-decoder attention!`

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$


<div align="center"><img src="./assets/scale dpa.png" width="70%"> </div>

**Multi-head attention**

将Q、K、V通过一个线性映射之后，分成 h 份，对每一份进行scaled dot-product attention效果更好。然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出

<div align="center"><img src="./assets/multihead attention.png" width="70%"> </div>


# 3. 常用方法

## 3.1 参数初始化

PyTorch 中参数的默认初始化在各个层的 `reset_parameters()` 方法中。例如: `nn.Linear` 和 `nn.Conv2D`，都是在 `[-limit, limit]` 之间的均匀分布，其中 `limit` 是 `1./sqrt(fan_in)`

- 如果将所有的权重都设置为0，则每一层激活函数的输出都是0
- 用N(0,1）标准正态的方式去初始化参数，每层激活函数的输出集中为-1或者1，这也就意味着激活函数陷入了饱和状态，在这个状态下，激活函数的导数为0
- 采用均匀分布的方式去初始化参数，结果随着层数的增加，每层激活函数的输出逐渐往0靠拢

### 3.1.1 Xavier Initialization

Xavier初始化的基本思想是保持输入和输出的方差一致，这样就避免了所有输出值都趋向于0。这是通用的方法，适用于任何激活函数。

**为什么均值为0，标准差为1这么重要?**

考虑一个有100层的深度神经网络。在每一步，权重矩阵乘以来自前一层的激活。如果每一层的激活大于1，当它们被重复乘以100次时，它们就会不断变大，爆炸到无穷大。类似地，如果激活值小于1，它们将消失为零。这叫做渐变爆炸和渐变消失问题。我们可以从下图中看到这一点。甚至比1稍大一点的值也会爆炸成非常大的数字，而比1稍小一点的值也会消失为零。


**xavier初始化方法中服从均匀分布 $U(-a, a)$**
$$a= gain * \sqrt\frac{6}{fan_{in} + fan_{out}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系

**xavier初始化方法中服从均匀分布 $N(0, std)$**
$$std= gain * \sqrt\frac{2}{fan_{in} + fan_{out}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系

### 3.1.2 Kaiming Initialization

**kaiming初始化方法中服从均匀分布 $U(−bound,bound)$**
$$bound=  \sqrt\frac{2}{(1+a^2) × fan_{in}}$$

`a` 是激活函数的负斜率（对于`leaky_relu`来说），如果激活函数是`relu`的话，`a` 为 `0`。`mode` 默认为fan_in模式，可以设置为fan_out模式。fan_in可以保持前向传播的权重方差的数量级，fan_out可以保持反向传播的权重方差的数量级

**kaiming初始化方法中服从均匀分布 $N(0, std)$**
$$std=  \sqrt\frac{2}{(1+a^2) × fan_{in}}$$

其中 `fan_in` 是 $H×W*C_{in}$, `fan_out` 是$H×W*C_{out}$, 增益值`gain` 是一个比例值，来调控输入数量级和输出数量级之间的关系


## 3.2 优化算法

- [Adam与SGD](https://www.cnblogs.com/jiangkejie/p/10561100.html)
- [Base](https://zhuanlan.zhihu.com/p/32230623)

定义: 待优化参数: $w$ ，目标函数: $ f(w)$，初始学习率 $\alpha$。而后，开始进行迭代优化。在每个epoch $t$: 

1. 计算目标函数关于当前参数的梯度:  $g_t = \nabla f(w_t)$
2. 根据历史梯度计算一阶动量和二阶动量: $m_t; V_t$
3. 计算当前时刻的下降梯度:  $\eta_t = \alpha 
\cdot m_t / \sqrt{V_t} $
4. 根据下降梯度进行更新: $w_{t+1} = w_t - \eta_t $  

### 3.2.1 SGD


#### 3.2.1.1 SGD
SGD没有动量的概念，因此
$$m_t = g_t; V_t=I^2$$

$$ \eta_t= \alpha 
\cdot g_t$$

SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点

#### 3.2.1.2 SGD with Momentum

为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量

$$m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t$$

一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 $\frac{1}{1-\beta_1}$ 个时刻的梯度向量和的平均值。

也就是说，t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。  的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。

#### 3.2.1.3 SGD with Nesterov Acceleration

NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进，改进点在于步骤1。我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：

$$g_t =  \nabla f(w_t - \alpha \cdot \frac{m_{t-1}}{\sqrt{V_{t-1}}})$$

然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤2中当前时刻的累积动量

### 3.2.2 AdaGrad

此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和

$$V_t=\sum_{\tau =1}^{t}g_{\tau}^{2}$$

此时实质上的学习率由 $ \alpha$ 变成了 $\frac{\alpha}{\sqrt{V_t}}$。 一般为了避免分母为0，会在分母上加一个小的平滑项。因此 $\sqrt{V_t}$ 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。

这一方法在稀疏数据场景下表现非常好。但也存在一些问题：因为 $\sqrt{V_t}$ 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。

### 3.2.3 AdaDelta / RMSProp

由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。

指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量:

$$V_t = \beta_2 * V_{t-1} + (1-\beta_2)g_{t}^{2}$$

### 3.2.4 Adam

SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——**Adaptive + Momentum**

SGD的一阶动量:

$$m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t$$

加上AdaDelta的二阶动量: 

$$V_t = \beta_2 * V_{t-1} + (1-\beta_2)g_{t}^{2}$$



## 3.3 卷积方式

### 3.3.1 Group Convolution


<div align="center"><img src="./assets/group conv.png" width="70%"> </div>

`Group Convolution` 顾名思义，则是对输入 `feature map` 进行分组，然后每组分别卷积。假设输入 `feature map` 的尺寸仍为 $C∗H∗W$，输出 `feature map` 的数量为 $N$个，如果设定要分成$G$个 groups，则每组的输入 `feature map` 数量为$\frac{C}{G}$，每组的输出 `feature map` 数量为$\frac{N}{G}$，每个卷积核的尺寸为$\frac{C}{G} * K * K$，卷积核的总数仍为 $N$ 个，每组的卷积核数量为 $\frac{N}{G}$，卷积核只与其同组的输入map进行卷积，卷积核的总参数量为 $N * \frac{C}{G} * K * K$，可见，总参数量减少为原来的 $\frac{1}{G}$，其连接方式如上图右所示，group1输出map数为2，有2个卷积核，每个卷积核的channel数为4，与group1的输入map的channel数相同，卷积核只与同组的输入map卷积，而不与其他组的输入map卷积。

1. **减少参数量**，分成GG组，则该层的参数量减少为原来的1G1G
2. **Group Convolution可以看成是structured sparse**，每个卷积核的尺寸由C∗K∗KC∗K∗K变为CG∗K∗KCG∗K∗K，可以将其余(C−CG)∗K∗K(C−CG)∗K∗K的参数视为0，有时甚至可以在减少参数量的同时获得更好的效果（相当于**正则**）。
3. 当分组数量等于输入map数量，输出map数量也等于输入map数量，即G=N=CG=N=C、NN个卷积核每个尺寸为1∗K∗K1∗K∗K时，Group Convolution就成了**Depthwise Convolution**，参见MobileNet和Xception等，**参数量进一步缩减**，如下图所示

<div align="center"><img src="./assets/depthwise conv.png" width="70%"> </div>

4. 更进一步，如果分组数 $G=N=C$，同时卷积核的尺寸与输入map的尺寸相同，即 $K=H=W$，则输出map为 $C∗1∗1$ 即长度为 $C$ 的向量，此时称之为 **Global Depthwise Convolution（GDC）**，见MobileFaceNet，可以看成是**全局加权池化**，与 **Global Average Pooling（GAP）** 的不同之处在于，GDC 给**每个位置赋予了可学习的权重**（对于已对齐的图像这很有效，比如人脸，中心位置和边界位置的权重自然应该不同），而GAP每个位置的权重相同，全局取个平均，如下图所示: 

<div align="center"><img src="./assets/GDC.png" width="70%"> </div>

### 3.3.2 Depthwise Convolution

深度可分离卷积主要分为两个过程，分别为逐通道卷积（Depthwise Convolution）和逐点卷积（Pointwise Convolution）。


#### 3.3.2.1 逐通道卷积（Depthwise Convolution）

Depthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积，这个过程产生的feature map通道数和输入的通道数完全一样。

Depthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。

#### 3.3.2.2 逐点卷积（Pointwise Convolution）

Pointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map。（卷积核的shape即为: **1 x 1 x 输入通道数 x 输出通道数**）

### 3.3.3 Dilated Convolution

网络中加入pooling层会损失信息，降低精度。那么不加pooling层会使感受野变小，学不到全局的特征。如果我们单纯的去掉pooling层、扩大卷积核的话，这样纯粹的扩大卷积核势必导致计算量的增大，此时最好的办法就是Dilated Convolutions（扩张卷积或叫空洞卷积）。

<div align="center"><img src="./assets/dilation conv.gif" width="70%"> </div>

dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv，比如图像分割、语音合成WaveNet、机器翻译ByteNet中。


## 3.4 BN，LN，IN，GN

- [Base](https://zhuanlan.zhihu.com/p/33173246)

往往用在深度神经网络中激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。

<div align="center"><img src="./assets/norm.png" width="70%"> </div>

- `BatchNorm`: batch方向做归一化，算 <font color="red">$N*H*W$</font> 的均值，对**小batchsize效果不好**；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布
- `LayerNorm:` channel方向做归一化，算 <font color="red">$C*H*W$</font> 的均值，主要**对RNN作用明显**；
- `InstanceNorm:` 一个channel内做归一化，算 <font color="red">$H*W$</font> 的均值，用在风格化迁移；因为在图像风格化中，生成结果主要**依赖于某个图像实例**，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
- `GroupNorm:` 将channel方向分group，然后每个group内做归一化，算 <font color="red">$\frac{C}{G}*H*W$</font> 的均值；这样与batchsize无关，不受其约束。
- `SwitchableNorm:` 将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。

**训练**时，均值、方差分别是**该批次**内数据相应维度的均值与方差；
**推理**时，均值、方差是基于**所有批次**的期望计算所得


### 3.4.1 为什么需要 Normalization

**独立同分布**的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，因此，在把数据喂给机器学习模型之前，[白化（whitening）](http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/)是一个重要的数据预处理步骤。白化一般包含两个目的: 

- 去除特征之间的相关性 —> 独立；
- 使得所有特征具有相同的均值和方差 —> 同分布。

**Internal Covariate Shift**

统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如，transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。

对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。

### 3.4.2 Normalization 的通用框架与基本思想

$$h = f(g*\frac{x-\mu}{\sigma}+b)$$

$\mu$ 是**平移参数**（shift parameter），$\sigma$ 是**缩放参数**（scale parameter）。通过这两个参数进行 shift 和 scale 变换。 $b$ 是**再平移参数**（re-shift parameter）， $g$ 是**再缩放参数**（re-scale parameter）。最终得到的数据符合均值为 $b$ 、方差为  $g^2$ 的分布。为了保证模型的表达能力不因为规范化而下降。

### 3.4.3 Normalization 为什么会有效？

**Normalization 的权重伸缩不变性**

权重伸缩不变性（weight scale invariance）指的是，当权重  按照常量  进行伸缩时，得到的规范化后的值保持不变。权重伸缩不变性可以有效地提高反向传播的效率。权重伸缩不变性还具有参数正则化的效果，可以使用更高的学习率。数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择。